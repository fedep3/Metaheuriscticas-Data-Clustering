% Marco Teorico.
\chapter{Definición del problema} 
\label{chap:dclust}

\section{Data clustering} 
\label{sect:dclust}

\subsection{Definición} 
\label{sect:dclustdef} \label{sect:dclustd}

    \emph{Data clustering} (agrupación no supervisada de datos, en español) es
el proceso de identificar el agrupamiento natural para datos multidimensionales,
basados en alguna medida de similitud \cite{DC_0}. Antes de dar un definición
formal al problema de \emph{data clustering}, es necesario definir los
siguientes términos que son usados repetidas veces a través de este proyecto de
grado\cite{SwAjAm2009}:

\begin{itemize}

\item {\bf Patrón o vector característico:} es un objeto del conjunto de datos
que se va a agrupar.
\item {\bf Característica o atributo:} es una componente de un patrón.
\item {\bf Cluster:} es una colección de patrones los cuales son similares entre
sí y diferentes a los patrones de otros clusters.
\item {\bf Hard clustering:} Cada patrón pertenece a un único cluster.
\item {\bf Fuzzy clustering:} Cada patrón pertenece a uno o más clusters con cierto grado de membresía.
\item {\bf Medida de Distancia:} es una métrica usada para evaluar la si\-mi\-li\-tud
entre patrones (ver sección \ref{sect:mdist}).
\item {\bf Centroide:} es un patrón representante de los miembros de un cluster.
\end{itemize}

 El problema de \emph{clustering} se puede definir formalmente como sigue \cite{DC_1}:

Sea $P = \{ Z_1, Z_2, \dots , Z_N\}$ un conjunto de $N$ patrones, 
donde cada uno tiene $M$ características. Éstos pueden
ser representados por una matriz $Z_{NxM}$. El vector de la fila $i$ ca\-rac\-te\-ri\-za al 
patrón $i$ del conjunto $P$ y cada elemento $Z_{i,j}$ en $Z_i$, su característica $j$.
Dada la matriz Z$_{NxM}$, el objetivo es que un algoritmo de \emph{clustering}
intente hallar el particionamiento $C = \{ C_1, C_2, \dots , C_K \}$ tal que los
patrones de un mismo cluster $C_i$ sean similares y disímiles a los patrones
de clusters diferentes:

\begin{itemize} 
\label{en:properties}

\item No pueden haber clusters vacíos:

$ C_i \neq \emptyset \;\; \forall\; i\; donde\; i\; \in\; \{1, 2, \dots, K\} $

\item Cada patrón debe ser asignado a un cluster:

$\bigcup_{i=1}^{K} C_i = P$

\item Cada patrón debe ser asignado a uno y sólo un cluster (sólo en el caso de
\emph{Hard-Clustering}):

$ C_i \cap C_j = \emptyset \;\; \forall\; i,j\; donde\; i,j\; \in \; \{1, 2, \dots, K\} \land i \neq j  $
\end{itemize}

Dado que el conjunto de datos puede ser particionado de varias maneras
conservando las propiedades de arriba, es necesaria 
una medida compruebe la bondad de la partición. El problema se convierte en hallar
una partición $C^*$ óptima o lo más cercano a ella, en comparación a 
las otras soluciones posibles $C = \{ C^1, C^2, \dots, C^{T(N,K)} \}$, donde 
$S(N,K) = { {\displaystyle\frac{1}{K!}} \cdot {\displaystyle\sum_{i=0}^{K} (-1)^i  \binom{K}{i} (K-i)^N} }$
es el número de particiones posibles. Esto es lo mismo que optimizar $f(Z_{NxM}, PC)$,
donde PC es un partición del conjunto C y $f(.)$ es una función que
cuantifica la calidad del particionamiento en base a la similitud o disimilitud
de los patrones.

\section{Medidas de similitud} 
\label{sect:mdist}

Una parte fundamental del proceso de \emph{clustering} es el de saber cuan
similares son dos patrones del conjunto de datos. Usualmente, se utilizan medidas
de distancia entre patrones. A continuación, se presentan algunas medidas de
distancia \cite{PSO_0}:
\begin{itemize}
    \item \textbf{Distancia Euclideana}: es la distancia ``ordinaria'' entre dos
puntos de un espacio euclídeo $M$-dimensional, siendo $M$ la cantidad de
componentes de cada vector (patrón):
\begin{equation}\label{mdist: euclidean}
    d(\overrightarrow{x_a}, \overrightarrow{x_b}) = \displaystyle\sqrt{\displaystyle\sum_{j = 1}^{M} (x_{a,j} - x_{b,j})^2} = \|\overrightarrow{x_a} - \overrightarrow{x_b}\|
\end{equation}
    \item \textbf{Métrica de Minkowsky}
\begin{center}
    $d^{\alpha}(\overrightarrow{x_a}, \overrightarrow{x_b}) = \left(\displaystyle\sum_{j = 1}^{M} (x_{a,j} - x_{b,j})^{\alpha}\right)^{1/\alpha} = \|\overrightarrow{x_a} - \overrightarrow{x_b}\|_{\alpha}$
\end{center}
siendo $\alpha$ la norma vectorial deseada.
    La distancia euclideana es un caso especial de esta métrica cuando $\alpha = 2$
y cuando $\alpha = 1$ se le conoce como la \textbf{Distancia Manhattan}
    \item \textbf{Similitud del Coseno}: mide la similitud entre dos
vectores (patrones), calculando el coseno del ángulo que se forma entre ellos:
\begin{center}
    $d(\overrightarrow{x_a}, \overrightarrow{x_b}) = \displaystyle\frac{\overrightarrow{x_{a}} \cdot \overrightarrow{x_{b}}}{\|\overrightarrow{x_a}\| \cdot \|\overrightarrow{x_b}\|}$
\end{center}
\end{itemize}

\section{Técnicas de clustering}
Muchos de los algoritmos de \emph{clustering} se basan en dos
técnicas populares conocidas como  \emph{clustering jerárquico} y
\emph{clustering particional} \cite{PSO_0}.

\subsection{Clustering jerárquico}
Los algoritmos en esta categoría generan
un árbol de cluster, también llamado \emph{dendograma}, al usar heurísticas para
separar clusters (algoritmos de división) o técnicas de unión de clusters
(algoritmos de aglomeración):
    \begin{itemize}
        \item \emph{Algoritmos de división}: inicialmente, estos algoritmos
tienen un sólo cluster con todos los patrones asignados a él. A través del
proceso de división, clusters grandes se subdividen en clusters más pequeños,
donde lo ideal es que la distancia entre patrones de un mismo cluster sea
mínima.
        \item \emph{Algoritmos de aglomeración}: estos algoritmos comienzan con
$n$ clusters, siendo $n$ la cantidad de patrones, de modo que cada patrón esté
en un cluster. A través del proceso de aglomeración, clusters similares son
unidos en un nuevo cluster.
    \end{itemize}

En general, las técnicas de \emph{clustering} jerárquico tienen las siguientes
ventajas \cite{PSO_0}:
    \begin{itemize}
        \item El número de clusters no tiene que ser especificado \emph{a priori}.
        \item Son independientes a las condiciones iniciales.
    \end{itemize}
Sin embargo, sus principales desventajas son \cite{PSO_0}:
    \begin{itemize}
        \item Son computacionalmente caros. Por lo tanto, no se adecúan a conjunto
de datos grandes \cite{DC_2}.
        \item Son estáticos. Por ejemplo, patrones asignados a un cluster no se
pueden mover a otro cluster.
        \item A veces, no podrán separar clusters que se solapen debido a la
falta de información acerca de la forma global y el tamaño de los clusters.
    \end{itemize}

\subsection{Clustering particional} 
\label{sect:cpart}
Estos algoritmos dividen el conjunto
de datos en el número de clusters especificados. Tratan de minimizar o maximizar
algún criterio, así que pueden ser tratados como problemas de optimización
generalmente \emph{NP-hard} y combinatorios \cite{DC_3}.

Las ventajas de los algoritmos jerárquicos son las desventajas de los algoritmos
particionales y \emph{viceversa} \cite{PSO_0}. Su principal ventaja es que no
son tan caros computacionalmente como las técnicas de \emph{clustering}
jerárquico. Por lo tanto, se adecúan a conjuntos de datos grandes \cite{PSO_0}.

El algoritmo de \emph{clustering} particional más usado es el \emph{K-means} \cite{DC_4}. 
Fue diseñado para datos numéricos en donde cada cluster tiene un centro llamado
media (\emph{mean}). Éste comienza con un número $K$ de clusters establecido.
Se inicializan los centroides y se irán cambiando los patrones de un cluster
a otro donde su distancia al centroide sea menor, hasta que cierta condición de
parada se cumpla\cite{GePo2010}.

El centroide (media) $\overrightarrow{m_i}$ de un cluster $C_i$ ($i \in [1, K]$)
se define del siguiente modo:
\begin{equation} \label{kmeans: mean}
\overrightarrow{m_i} = {1 \over |C_{i}|} \sum_{\overrightarrow{z} \in C_{i}}\overrightarrow{z}
\end{equation}

El algoritmo general del K-means se muestra en el pseudo-código \ref{Kmeans}.

\begin{lstlisting}[float=!h, caption=Algoritmo General K-means\cite{GePo2010}, label=Kmeans]
- Inicializar los $K$ centroides.
- Mientras no se cumpla el criterio de parada:
    - Calcular las distancias de cada patrón a cada uno de los centroides.

    - Para cada $i \in [1, N]$:
        - Asignar el patrón $\overrightarrow{z}_i$ al cluster $C_k$, si la distancia entre
          el patrón $\overrightarrow{z}_i$ y el centroide $\overrightarrow{m}_k$ es mínima, $\forall k \in [1, K]$.
      Fin.

    - Para cada $k \in [1, K]$:
        - Actualizar centroide $k$ con la ecuación (%\ref{kmeans: mean}%).
      Fin.
  Fin.
\end{lstlisting}

La inicialización de los centroides para el algoritmo \emph{K-means} puede
hacerse de manera aleatoria o a través de un método más sofisticado, como por
ejemplo algún algoritmo \emph{greedy}.

Sus propiedades más importantes son las siguientes\cite{GePo2010}:
\begin{itemize}
    \item Es bastante eficiente para hacer \emph{clustering} de grandes conjuntos de datos,
ya que su complejidad es lineal al número de datos ($O(N)$).
    \item Tiende a quedar atrapado en óptimos locales.
    \item Sólo sirve para datos numéricos.
    \item Su buen funcionamiento depende de los centroides iniciales.
\end{itemize}

\section{Técnicas de validación de clusters} 
\label{sect:tval}
El principal objetivo de la validación de clusters es evaluar los resultados
del proceso de \emph{clustering} para encontrar la mejor partición del conjuntos
de datos.

Dos criterios que han sido ampliamente considerados como suficientes al
medir la calidad de la partición de los datos son\cite{PSO_0}:
\begin{itemize}
    \item \emph{Compactación}: los patrones dentro de un cluster deben ser
similares entre sí y diferentes de los patrones de otros clusters. Mientras
menos varianza exista entre las distancias de los elementos de un cluster, más
compacto es el mismo.
    \item \emph{Separación}: los cluster deben estar bien diferenciados. No
deberían existir clusters con centroides iguales o parecidos.
\end{itemize}

Para esta labor se utilizan los llamados \emph{índices de validez}:

\begin{itemize}

\item \textbf{Índice Davis-Bouldin (\emph{DB})}:
    Esta medida es una función de la proporción de la suma de la dispersión dentro 
de los clusters y la separación entre clusters \cite{DC_5}:

\begin{equation} \label{mdist: db}
DB(K) = \frac{1}{K} \displaystyle\sum_{i = 1}^K \left[\displaystyle\max_{j \in K, j \neq i}\left( \frac{S_{i,q} + S_{j,q}}{d_{ij,t}}\right)\right]
\end{equation}
donde,
\begin{itemize}
    \item $q,t \geq 1$; Ambos son enteros e independientes entre sí.

    \item $S_{i, q}$ es la dispersión dentro del $i$-ésimo cluster (distancia
          \emph{intra-cluster}) y viene dado por:

            \begin{center}
            $S_{i,q} = \left( \frac{1}{N_i} \displaystyle\sum_{\overrightarrow{x} \in C_i} \|\overrightarrow{x} - \overrightarrow{m_i}\|_{2}^q\right)^{1/q}$
            \end{center}
            donde,
                \begin{itemize}
                    \item $\overrightarrow{m_i}$ es el centroide del $i$-ésimo cluster.
                    \item $N_i$ es el número de elementos en el $i$-ésimo cluster $C_i$.
                \end{itemize}

    \item $d_{ij, t}$ es la distancia entre el $i$-ésimo y el $j$-ésimo cluster
          (distancia \emph{inter-cluster}) y viene dado por:

            \begin{center}
            $ d_{ij,t} = \left(\displaystyle\sum_{p=1}^{M} | m_{i,p} - m_{j,p} |^t \right)^{1/t} = \|\overrightarrow{m_i} - \overrightarrow{m_j}\|_t$
            \end{center}
\end{itemize}

    Por lo tanto, mientras mejor sea una partición menor será el valor del índice
$DB$ asociado a la misma:
\begin{itemize}
    \item Mientras menor distancia \emph{intra-cluster} exista, mejor formado
estará el cluster, es decir, las distancias $S_{i,q}\text{, }\forall i \in \{1, \cdots, K\}$
deben ser lo más pequeñas posibles. Si los centroides de todos los clusters son
iguales a los patrones que pertenecen al cluster al que representan, entonces el
índice $DB$ tenderá a cero.

    \item Mientras mayor distancia \emph{inter-cluster}, mejor diferenciados
estarán los diferentes clusters, es decir, las distancias
$d_{ij, t}\text{, }\forall i,j \in \{1, \cdots, K\}$ deben ser lo más grandes
posibles. Si los centroides de de dos clusters son iguales, entonces el índice
$DB$ tenderá a infinito. 
\end{itemize}

\item \textbf{Índice Chou-Su (CS)}: Chou et al. \cite{SwAjAm2009} proponen esta
métrica como forma de determinar la bondad de una partición. Para su evaluación,
hacen falta calcular los centroides de cada grupo y alguna medida de distancia
entre vectores.

%Centroide por promedio.
%INICIO
\begin{center}
$ CS(K) = \displaystyle\frac{\displaystyle\sum_{i=1}^K 
                \left(\frac{1}{N_i}\displaystyle\sum_{\overrightarrow{x} \in C_i} \max_{\overrightarrow{y} \in C_i} \{ d(\overrightarrow{x}, \overrightarrow{y})\}\right)}{\displaystyle\sum_{i=1}^K \left(\displaystyle\min_{j \in K, j \neq i} \{ d(\overrightarrow{m_i}, \overrightarrow{m_j})\} \right)}$
\end{center}
donde el numerador indica la distancia máxima promedio entre los patrones
pertenecientes a los clusters y el denominador representa la distancia mínima
promedio entre los centroides de los clusters. Una buena partición debe tener un
numerador mínimo (distancia \emph{intra-cluster} pequeña) y un denominador máximo
(distancia \emph{inter-cluster} grande). Por lo tanto, a menor $CS$, mejor
partición.

La medida $CS$ es más eficiente en abordar clusters de diferentes densidades y/o
tamaños que las otras medidas de validez más populares \cite{DC_6}. Sin embargo,
la carga computacional que se incrementa cuando aumenta el número de clusters
($K$) y la cantidad de objetos ($N$).

\item {\bf Cuantificación del Error}: La forma más general de medir la calidad
de la partición es la cuantificación del error y se define como \cite{PSO_0}:
\begin{equation}\label{cluster: je}
J_e = \displaystyle\frac{\displaystyle\sum_{k = 1}^K \left( \displaystyle\sum_{\overrightarrow{x} \in C_k} d(\overrightarrow{x}, \overrightarrow{m_k}) \right) / N_k}{K}
\end{equation}

    Mientras menor dispersión exista dentro de los clusters, más compactos
serán. Por lo tanto, a menor error $J_e$, mejor partición.
\end{itemize}
