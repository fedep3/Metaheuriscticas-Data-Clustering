% Marco Teorico.
\chapter{Marco te'orico} \label{chap:ssimilar}

\vspace{5 mm}

\section{Data Clustering} \label{sect:dclust}

\subsection{Introducci\'on} \label{sect:dclusti}

Data clustering (o s\'olo clustering), tambien llamado an\'alisis de clusters, an\'alisis de segmentaci\'on,
an\'alisis de taxionom\'ia, o clasificaci\'on no supervisada, es un m\'etodo de crear grupos de objetos,
o clusters, de tal manera que cada los objetos dentro de un cluster sean similares y los objetos 
en clusters distintos sean diferents. \cite{GaChJi2007}

En \cite{SwAjAm2009} hablan de ciertos puntos importantes:

\begin{itemize}

\item Hay muchas definciones porpuestas por una diversa cantidad de personas,
 lo que demuestra la dificultad de proveer una \'unica definici\'on formal. Ésto radica en que
es bastante complejo capturar por los medios de cualquier criterio individual que se 
use la noci\'on que tiene un humano. El siguiente ejemplo va a poner en claro.

Considere los siguientes animales: oveja, perro, gato (mam\'iferos), gorri\'on, gaviota (aves),
v\'ivora, lagarto (rept\'iles), pez de colores, salmonete, tibur\'on azul (peces), y rana (anf\'ibio).
Para organizar estos animales en clusters, se tiene que definir un criterio. Por ello, si 
empleamos la manera en que estos animales llevan a cabo su descendencia, la oveja, perro, gato
y tibur\'on azul van a ser asignados al mismo cluster, mientras que el resto van a dormar un segundo
(Figura \ref{fig:ejemplo1}).  En cambio, si el criterio es la existencia de pulmones, 
el pez de colores, el salmonete y el tibur\'on son
asignados al mismo cluster, mientras que el resto a otro (Figura \ref{fig:ejemplo2}).
 Por otro lado, si el criterio es el ambiente
donde viven los anoamles, la oveja, perro, gato, gorri\'on, gaviota, v\'ivora y el lagarto van a formar un cluster
(viven fuera del agua), el pez de colores, salmonete y el tibur\'on azul van a formar un otro (viven afuera
del agua), y un tercero que va a contener a la rana, ya que puede vivir en los dos (Figura \ref{fig:ejemplo3}). 

\begin{figure}[htb]
\centering
\includegraphics[scale=0.35,type=png,ext=.png,read=.png]{figures/ejemplo1}
\caption{Clustering usando como criterio la manera que llevan a cabo su descendencia}
\label{fig:ejemplo1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.35,type=png,ext=.png,read=.png]{figures/ejemplo2}
\caption{Clustering usando como criterio la existencia de pulmones}
\label{fig:ejemplo2}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.35,type=png,ext=.png,read=.png]{figures/ejemplo3}
\caption{Clustering usado como criterio el ambiente donde viven}
\label{fig:ejemplo3}
\end{figure}

Se puede ver que gracias a la falta de un criterio universal para el clustering,
 \'esta es muy subjetiva en muchos casos.

\item Se puede ver desde el punto de vista de aprendizaje autom\'atico, donde los clusters
correponden a patrones escondidos en los datos, la b\'usqueda de clusters 
es una especie de aprendizaje no supervisado, y el sistema resultante representa
un concepto de datos. Es importante entender la diferencia
entre clustering (aprendizaje no supervisado) y clasificaci\'on supervisada 
(an\'alisis discriminante). En esta \'ultima, se provee una colecci\'on patrones etiquetados
(preclasificados); el problema es etiquetar a un patr\'on recien encontrado, a\'un no etiquetado. 
Tipicamente, los patrones etiquetados (entrenamiento) ya dados son usados para obtener
la descripciones de las clases, que a su vez se utilizan para etiquetar un nuevo patr\'on.
En este caso de clustering, el problema consiste en agrupar una colecci\'on de patrones no etiquetados
en clusters significativos. En este sentidos, las etiquetas se asocian con los clusters tambi\'en,
pero esta categor\'ia de etiquetas son basados en datos, es decir, que se obtienen unicamente
de \'estos.

\item Un algoritmos de clustering se espera que descubra el agrupamiento natural
(pertinente a la noci\'on de los humanos clustering) que existe en una conjunto 
de patrones o puntos de datos. Cada patr\'on puede ser identificado como un punto en un hiperespacio,
llamado espacio de caracter\'isticas, abarcado por las caracter\'isticas asociadas a \'el. La entrada
es un conjunto esos puntos en el espacio carater\'istico multidimensional. Un algoritmo ideal de 
clustering preciso debe presentar como su salida, la etiqueda para cada patr\'on, es decir,
el cluster a cual pertenece cada punto.

\end{itemize}

\'Este problema a sido abordado por diversos campos del conocimiento como la estad\'istica
(an\'aslisis multivariado), teor\'ia de grafos, computaci\'on evolutiva, redes neurales y
as\'i sucesivamente\cite{SwAjAm2009}. Entre sus aplicaciones se encuentra la miner\'ia de datos, expresi\'on de los genes, 
segmentaciones de clientes y procesamiento de im\'agenes, muchas otras. \cite{GaChJi2007}

En especial llama la atenci\'on el \'ultimo mencionado. El procesamiento de im\'agenes es fundamental para los
humanos. Su importancia radica en que la salida puede ser usada como la entrada para 
un modelo basado en sistemas de reconocimiento de objetos.
\'Esto le da una gama de aplicaciones gigantescas: cualquier tipo de reconicimiento de im\'agenes.
Tambi\'en en el \'area m\'edica puede ayudar a encontrar regiones en las im\'agenes que sean tumores,
que posiblemente humano no logre identificar, lo que es extremadamente
\'util.

Brucker en \cite{Br1978} ilustr\'o que el problema es NP-hard cuando el n\'umero 
de clusters excede 3.

\subsection{Definici\'on} \label{sect:dclustdef}

Siguiendo con \cite{SwAjAm2009} antes de dar un definici\'on matem\'atica al problema hay que hablar de 
los siguientes t\'erminos que van a ser usados a trav\'es de la tesis:

\begin{itemize}

\item {\bf Patr\'on o vector caracter\'istico:} Se va a encargar de abstraer matem\'aticamente las
caracter\'isticas que poseen los objetos a los cuales se les har\'a el clustering.

\item {\bf Caracter\'istica:} Es una componente de un patr\'on. \'Esta representa
la base usada para clasificaci\'on de los patrones.

\item {\bf Cluster:} Es un grupo de patrones similares, y los patrones de dos clusters 
distintos no deben ser similares.

\item {\bf Hard clustering:} Cada patr\'on se asigna solamente a un cluster.

\item {\bf Medici\'on de distancia:} Es la m\'etrica con la cual se va a evaluar
la disimilaridad entre los patrones. M\'as adelante se habl\'a en m\'as detalle.

\end{itemize}

Ahora se puede dar una definici\'on formal al problema:

Sea $P = \{ P_1, P_2, \dots , P_N\}$ un conjunto de N patrones, 
donde cada uno tiene M caracter\'isticas. Éstos pueden
ser reprentados por una matriz $Z_{NxM}$. El vector de la fila $i$ caracteriza al 
patr\'on $i$ del conjunto $P$ y cada elemento $Z_{i,j}$ en $Z_i$ su caracter\'istica $j$.
Dada tal matriz Z$_{NxM}$ la idea es que un algoritmo de clustering intente hallar
el particionamiento $C = \{ C_1, C_2, \dots , C_K \}$ tal que los patrones
en el mismo cluster $C_i$ su similaridad sea la mayor y entre clusteres diferentes
sean lo más disimilar:

\begin{enumerate}

\item Cada cluster debe tener por lo menos un patr\'on asignado:

$\forall i | i \in \{1, 2, \dots, K\} : C_i \neq \emptyset$

\item Dos patrones distintos no tienen ning\'un patr\'on en com\'un:

$\forall i,j | i,j\in \{1, 2, \dots, K\} \land i \neq j:  C_i \cap C_j \neq \emptyset$

\item Cada patr\'on debe estar asignado a un cluster:

$\bigcup_{i=1}^{K} C_i = P$

\end{enumerate}

Dado que el conjunto de datos dados puede ser partidionado de varias maneras
manteniendo las propiedades de arriba, es necesaria una función objetico o en otras palabras
una medida de que tan buena es la partici\'on. El problema se comvierte en hallar
una partici\'on $C^*$  \'optima o lo m\'as cercano a ella en comparaci\'on a 
las otras soluciones posible $C = \{ C^1, C^2, \dots, C^{T(N,K)} \}$ donde 
$T(N,K) = { {1 \over K!} \times {\sum_{i=1}^{K} (-1)^i  \binom{K}{i} (K-i)^i} }$
es el n\'umero de particiones posibles. \'Esto es lo mismo que optimizar $f(Z_{NxM}, PC)$,
donde PC es un partici\'on del conjunto C y $f$ es una funci\'on objetivo que
cuantifica la calidad del particionamiento en base a la similaridad o disimilaridad
de los patrones.

\section{Metaheur\'isticas}

\subsection{Abeja}

\subsection{DE}

Es un algoritmo basado en poblaci\'on de
optimizaci\'on global que hace uso de una representaci\'on de punto flotante
(codificaci\'on real), bastante parecido al gen\'etico. Se tienen los pasos de cruce 
y selecci\'on, pero no mutaci\'on.
El $i$-\'esimo vector individual (cromosoma)
de la poblaci\'on con tiempo (generaci\'on) $t$ tiene $d$ componentes
dimensiones:

%Ejemplo de vector.
%INICIO
\begin{center}
$ \overrightarrow{Z_i}(t) = [ Z_{i,1}(t), Z_{i,2}, \cdots, Z_{i,d}(t) ] $
\end{center}
%END

Para cada vector individual $\overrightarrow{Z_k}(t)$ que pertenece
a la poblaci\'on actual, el DE aleatoriamente toma tres individuos
$\overrightarrow{Z_i}(t)$, $\overrightarrow{Z_j}(t)$ y $\overrightarrow{Z_m}(t)$ de la misma generaci\'on (de modo que sean distintos $k$, 
$i$, $j$ y $m$). Entonces calcula la diferencia entre $\overrightarrow{Z_i}(t)$ y $\overrightarrow{Z_j}(t)$, lo escala por un escalar $F$
(usualmente $F \in [0, 1]$) y crea un hijo prueba $\overrightarrow{U_i}(t + 1)$ a\~nadiendo el resultado a $\overrightarrow{Z_m}(t)$. De modo
que para la $n$-\'esima componente del vector:

\[
  U_{k,n}(t+1) =
  \begin{cases}
    Z_{m,n}(t) + F(Z_{i,n}(t) - Z_{j,n}(t))  & \text{si } rand_n(0,1) < Cr\\
    Z_{k,n}(t)                               & \text{sino}
  \end{cases}
\]

Donde $Cr \in [0, 1]$ es un escalar que es par\'ametro de el algoritmo,
llamado la \emph{tasa de cruce}. Si el nuevo hijo tiene mejor valor
con la funci\'on objetivo, entonces reemplaza al padre en la siguiente
generaci\'on, sino, el padre entonces se queda en la misma:
\[
  \overrightarrow{Z_i}(t+1) =
  \begin{cases}
    \overrightarrow{U_i}(t+1) & \text{si } f(\overrightarrow{U_i}(t+1)) > f(\overrightarrow{Z_i}(t)) \\
    \overrightarrow{Z_i}(t)   & \text{si } f(\overrightarrow{U_i}(t+1)) \leq f(\overrightarrow{Z_i}(t))
  \end{cases}
\]
donde $f$ es la funci\'on objetivo a ser maximizada.

\cite{SwAjAm2008}

\subsection{Gen\'etico}

\subsection{Hormiga}

Hormiga es una metaheurística usada para resolver problemas combinatorios
difíciles. \'Esta se inspira en los diversos comportamientos de las hormigas.
Comunmente se basa en el rastro de feromona y el comportamiento de seguirlo 
de éstas, usado en la búsqueda de comida. Una hormiga que se est\'a moviendo 
suelta feromonas en el suelo, as\'i marcando un camino. \'Este químico, que 
desaparece con el tiempo, el reforzado si otras hormigas usan ese mismo camino.
Por lo tanto, las mejores v\'ias incrementan su nivel de feromonas con el tiempo, 
y al contrario con los peores. Fue propuesto por Marco Dorigo en 1992, 
y lo ha expandidos en sus trabajos posteriores.
\cite{GePo2010} \cite{Le2007}

Su funcionamiento es el siguiente: primero, $m$ hormigas contruyen
soluciones del problema, sesgada por la informaci\'on de las feromonas y posiblemente
por las disponible por parte de las heur\'isticas. Una vez que las hormigas hallan completado
sus soluciones, se pueden mejorar mediante una b\'usqueda local. Finalmente,
antes de empezar con la siguiente iteraci\'on , los ratros de feromonas son
actualizados para reflejar la experiencia de b\'usqueda de las hormigas:

\begin{lstlisting}[mathescape, language=Pascal]
  Incialización.
  Mientras no se cumpla el criterio de parada.
    Construcción de las soluciones.
    Aplicar búsqueda local.
    Actualizar feromonas.
\end{lstlisting}

\begin{itemize}

\item {\bf Inicializaci\'on:} Los par\'ametros son establecidos y todas las variables 
de feromonas son puestas en t$_{0}$, el cual es un par\'ametro del algoritmo.

\item {\bf Construcci\'on de las soluciones:} Cada hormiga empieza con una soluci\'on vac\'ia
$s_p = \emptyset $. En cada paso de las construcci\'on , una hormiga
extiende su soluci\'on parcial actual $s_p$ eligiendo un posible componente $c_i^j \in N(s_p) \subseteq C$ 
y agregandolo a \'esta. $N(s_p)$ es el conjunto de los
componentes de soluci\'on que pueden ser agregados manteniendo su validez
y es definido implicitamente por el proceso de construcci\'on de soliciones
que las hormigas implementan. 

La elecci\'on del componente de la soluci\'on que se quiere agregar es 
hecha probabil\'isticamente en cada paso de la construcci\'on. La manera m\'as
com\'un es la siguiente:

\[
p(c_i^j) = {\tau_{ij}^{\alpha}  \times [ \eta (c_i^j) ]^{\beta}}  \over { \sum_{c_i^l \in N(s_p)} \tau_{il}^{\alpha} \times [\eta (c_i^l)]^{\beta}] } 
\]

donde $\eta(.)$ es una funci\'on  que asigna a cada posible componente 
de la soluci\'on $c_i^j \in N(s_p)$ un valor heur\'istico, que es usualmente
llamada infomaci\'on heur\'istica. Los par\'ametos $\alpha$ y $\beta$ 
determinan la relativa influencia de los rastros de feromonas e informaci\'on
heur\'istica, por lo tanto influyendo significativamente en el comportamiento
del algoritmo.


\item {\bf Aplicar b\'usqueda local:} Una vez obtenidas las soluciones candidatos,
estas puede ser mejoradas aplicando algoritmos de b\'usqueda local.

\item {\bf Actualizaci\'on de las feromonas:}: tiene como objetivo
hacer que los compentes de una buena soluci\'on, 
sean m\'as deseables para las hormigas en las siguientes iteraciones. Hay esencialemente
dos mecanismos para lograr este objetico. El primero es el dep\'osito de feromonas,
el cual incrementa el nivel de feromonas de los coponentes de una soluci\'on
que est\'an asociados con un conjunto selccionado $S_{upd}$ de buenas soluciones.
El segundo es la evaporaci\'on del rastro de feromonas, el cual es un mecanismo
que decrece a medida que el tiempo pasa el dep\'osito de feromonas. Desde el punto de
vista pr\'actico, \'este es necesario para evitar la r\'apida convergencia del algoritmo
en una regi\'on sub\'optima. Es comunmente implementada de la siguiente
manera:

\[
\tau_{ij} = (1-\rho)\tau_{ij} + \sum_{s \in S_{upd} \land c_i^j \in s} g(s)
\]

donde $S_{upd}$ es el conjunto de soluciones usadas para depositar feromonas, 
$\rho in (0,1]$ es un par\'ametro llamada constante de evaporaci\'on,
$g(.):S \rightarrow  \Re^+$ es una funci\'on que determina la calidad
de la soluci\'on.
\end{itemize}

Actualmente los cient\'ificos est\'an empezando a inspirarse en 
la manera que las hormigas crean sus cementerios: limpian sus nidos
y crean pilas de cad\'averes. 
Tambi\'en de la organizaci\'on de cr\'ias, donde son agrupadas
de acuerdo a su tama\~no. El principio recae en la atracci\'on
entre los objetos transportados. Los clusters pequeños de objetos
similares van creaciendo atrayendo a las hormigas a despositar m\'as
objetos de acuerdo con su tama\~no o tipo. Este feedback positivo
conlleva a la formaci\'on de clusters homog\'enos.

El pioner en este trabajo es Deneubourg et al., donde aplican el
m\'etodo para tareas en rob\'otica. Este ha sido modificado por Lumer
y Faita para extenderlo a an\'alisis num\'erico de datos.
En estos algoritmos los datos son dispersados aleatoriamente
en un grid de dos dimensiones. Cada hormiga se mueve aleatoriamente
dentro de \'este agarrando y soltando estos datos. La decisi\'on
de agarrar o soltar un dato es aleatoria,
pero es influenciada por los datos en el vecindario, 
causando que datos similares tengan m\'as probabilidad de
estar juntos. La probabilidad de soltar un dato incrementa
en zonas de mayor densiadad de datos similares, y decrementa
cuando sucede el contrario.  En contraste
la probabilidad de agarrar un dato incrementa en las zonas de
menor densidad y decrementa en el opuesto.
\'Esta est\'an dadas por:

\[
P_p(i) = {k_1 \over {k_1 + f(i)}}
\]

\[
P_d(i) = 
  \begin{cases}
    2f(i) & \quad \text{si $f(i)<k_2$}\\
    1     & \quad \text{si $f(i) < k_2s$}\\
  \end{cases}
\]

\[
f(i) =
  \begin{cases}
    {{1} \over {s^2}} \sum_{j \in R(r(i))} {{1 - {d(i,j)}} \over \alpha} & \quad \text{si $f > 0$}\\
    0     & \quad \text{sino}
  \end{cases} 
\]

Donde $r(i)$ es la posici\'on del dato $i$ dado en el grid y $f(i)$ es una medida
del promedio de disimilaridad del dato $i$ con respecto a los otros $j$
presentes en su vencindario $R$ con tama\~no $s \times s$. $\alpha$
es la escala de disimilaridad y es clave en la ejecuci\'on del algoritmo:

\[
\alpha = {{1 \over {N(N-1)}} \sum_{i=1}^N \sum_{j=1}^N d(p_i,p_j)}
\]

\cite{OuBa2007}

El n\'umero de aplicaciones es bastante grande: resolver problemas
desde data clustering, programaci\'on (scheduling), balanceo
de l\'inea de equilibrio, TSP probabil\'istico, secuenciaci\'on de 
ADN, etc. \cite{GePo2010}


\subsection{PSO}
