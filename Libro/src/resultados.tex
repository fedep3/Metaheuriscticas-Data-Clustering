% Resultados
\chapter{Implementaci\'on y Resultados} \label{chap:impresultados}

%Puedes quitar esto(es opcional)
\vspace{5 mm}

\section{Lenguaje usado}  \label{sec:lusado}

El proyecto fue hecho usando C++. Se elegi\'o \'este debido 
a su eficiencia, ya que compila a c\'odigo de m\'aquina y permite una buena
abstraci\'on de los problemas con las clases. Adem\'as, su librer\'ia ya contiene bastantes
cosas ya hechas, por lo que va a ahorrar tiempo de programaci\'on.

El dise\~no de las clases es el siguiente:

\begin{figure}[htb]
\centering
\includegraphics[scale=0.35]{figures/clases.eps}
\caption{Dise\~no de clases}
\label{fig:jclases}
\end{figure}

Donde Metaheuristics es una clase abstracta, que posee las m\'etodos que seran
usados por las diversas metaheur\'isticas hijas. En especial tienen dos 
importantes: initialize y run. El primero se va a encargar de inicializar el algoritmo
para que luego sea ejecutado. Por ejemplo en el gen\'etico inicializa aleatoriamente
las soluciones. Y el run es en s\'i el que va a ejecutar el algoritmo haciendo 
uso de los par\'ametros y variables ya inicilizadas.

Por el otro lado Reader tambi\'en
es una clase abstracta y es la que se encarga de definir lo m\'etodos que van a 
tener sus descendientes a la hora de leer un archivo, que se implement\'o para 
tres tipos: csv, png y tiff.

\section{M'aquina de prueba} \label{sect:testbed}

Todas las pruebas fueron realizadas con el computador cuyas caracter'isticas
aparecen en el cuadro \ref{tb:testbed}. La herramienta {\tt mhs} fue
compilada sobre esa m'aquina, exclusivamente con optimizaci'on gcc de nivel 3,
que es el nivel m\'as alto que posee gcc y aplica todas las mejoras
que este posee.

\begin{table}[htb]
\footnotesize
\begin{center}
\begin{tabular}{|>{\columncolor{lightgray}}c|c|}
\hline
CPU & Intel Core I5 650 \\
\hline
RAM & 6 GB \\
\hline
Distribuci'on & Ubuntu 11.04 x86\_64 bits \\
\hline
Kernel Linux & 2.6.38-8-generic \\
\hline
Versi'on gcc & 4.5.2 \\
\hline
\end{tabular}
\caption{Computador usado para las pruebas}
\label{tb:testbed}
\end{center}
\end{table}

\section{M\'etricas usadas}  \label{chap:meusada}

Para medir la distancia entre dos patrones se va a usar la euclidiana que
ya fue explicada en \ref{sect:mdist}. Se elegi\'o esta debido
a su f\'acil implementaci\'on, buenos resultado y rapidez.

La funci\'on objetivo principal es la DB, comentada en \ref{sect:fobjetivo}.
Su baja complejidad algor\'itmitca en comparaci\'on con las dem\'as existentes y
su eficacia en medir bien las dos cosas que se buscan en los cluster (distancia
intracluster baja e intercluster alta) fueron los motivos para su elecci\'on. 

El DE y PSO tienen la tercera comentada en \ref{sect:fobjetivo}.

Para comparar que tan bueno es un algoritmo en comparacaci\'on a los otros, todos son evaluados al final con $1/DB(K)$, que 
es la forma de maximizaci\'on de la m\'etrica DB, y la del error $J_e$.

\section{Ajuste de par\'ametros}  \label{chap:ajustep}

Para cada metaheut\'istica hay que ver la influencia que sus par\'ametros 
poseen, buscar los m\'as adecuados y dar los resultados obtenidos.
Para ellos los pruebas se har\'an en dos pasos:

\begin{enumerate}

\item Hacer pruebas con todas las metaheur\'isticas variando los par\'ametros 
en todo su rango.

\item Con estos resultados, hacer un an\'alisis ANOVA para ver la influencia de cada
uno y de esta forma crear otras pruebas mucho m\'as espec\'ificas.

\end{enumerate}

\section{K-means}  \label{chap:ikmeans}

\'Este fue implementado tal cual como se explic\'o en \ref{sect:kmeans}. La
inicializaci\'on es totalmente aleatoria. Y la condici\'on de paradas
va a ser hasta que no se mejores un n\'umero de veces dado.

\section{Bee}  \label{chap:ibee}

\section{Gen\'etico}  \label{chap:igenetico}

Se leyeron todas las posbibles formas propuestas por \cite{HrCaFr2009} y se decidi\'o
lo siguiente:

\begin{itemize}

\item Los cromosomas van a ser los centroides de los clusters, de un tama\~no de $K$
dado. Esto permite que las operaciones que se hagan sobre los individuos tengan
un efecto mas fuerte sobre el cluster y trabajen de manera mas global: sobre cluster y
no con objetos.

\item La mutaci\'on no es exclusiva de los hijos como se propuso en el marco te\'orico,
sino que es a cualquier miembro de la poblaci\'on. Son tres posibles: dividir un cluster
en dos, creando uno nuevo, unir dos cluster, y cambiar un elemento de un cluster a otro.
De esta forma hay una buena variedad y buenos cambios que pueden lograr obtener buenos
resultados.

\item La selecci\'on es por torneo, donde al comienzo del algoritmo se especifica 
el tama\~no de \'esta.

\item La condici\'on de parada es un n\'umero dado de veces que no se mejore lo 
que se tiene.

\item El resto del algoritmo sigue tal cual como ya se explic\'o.

\end{itemize}

\section{DE}  \label{chap:ide}

Se bas\'o en los trabajos hecho por \cite{SwAjAm2008} y \cite{OmEnSa2005}, especialmente
de \'este \'ultimo.

Se requiere un $K$ de antemano y los individuos van a ser arreglos de $K$ centroides.
El algoritmo funciona tal cual como se explic\'o en \ref{sect:metade}. Una de las diferencias es
el cruce, donde adicionalemente se tiene
un punto entre $[1,M|$ donde es 100\% de probabilidad de cruce. La condici\'on
de parada es un n\'umero de iteraciones dado. Con respecto
a los par\'ametros $F$ y $Cr$, pueden ser establecidos o funcionar como se propone
en el primer art\'iculo, con ciertas modificaciones:

\begin{itemize}

\item {\bf $F$:} En cada iteraci\'on se calcula un nuevo $F$, con la siguiente expresi\'on:
$0.5 \times (1 + rand(0,1))$, donde $rand(0,1)$ es un n\'umero aletorio entre $[0,1]$.
Es claro que la media va a ser 0.75. \'Esto permite variaciones estoc\'asticas en el vector diferencia, y
por lo tando ayuda a mantener diversa la poblaci\'on a medida que la b\'usqueda progresa.

\item {\bf $Cr$:} Se establece un $Cr_{max}$ y $Cr_{min}$. La idea es que el $Cr$ vaya
decrementando linealmente desde el primero hasta el segundo. Al comienzo
se desea que explorar el espacio de b\'usqueda y al final se enfoquarse y mejorar las soluciones
que ya se llevan. Es f\'acil ver a medida que el $Cr$ disminuye, los hijos heradan m\'as
del padre. Se tomaron $Cr_{max} = 1.0$ y $Cr_{min} = 0.5$. La variaci\'on esta expresada por:

\[
(Cr_{max}-Cr_{min}) \times {{maxit - it} \over maxit} + Cr_{min}
\]

donde $maxit$ es el n\'umero de iteraciones tope dado al algoritmo e $it$ es la iteraci\'on actual.

\end{itemize}

\section{PSO}  \label{chap:ipso}

\section{Hormiga}  \label{chap:ihormiga}

Como se mencion\'o en \ref{sect:metaant} hay varias visiones de esta metaheur\'istica.
La primera es la que se basa en la b\'usqueda de comida por parte de las hormigas.
\'Esta es inviable para hacer clustering porque no se puede representar 
una soluci\'on mediante un grafo. Por ello es usada la otra perspectiva basada en c\'omo
las hormigas arman sus cementerios u organizan de las c\'rias. 

En \'esta las hormigas se van a mover en un grid de dos dimensiones. Si 
el tama\~no es muy peque\~no van a hacer muchos movimientos en vista que
no van a conseguir espacios vacios donde soltar el objeto que cargan. En cambio,
si es muy grande se haran movimientos innecesarios cuando la hormiga no lleva nada.
No hay una gu\'ia te\'orica del tama\~no adecuado. Por ello la implemetaci\'on que
se hizo fue basada de \cite{OuBa2007}, donde se propone usar un arreglo 
de $N/4$ c\'elulas en el cual las hormigas se pueden mover libremente.
Una de las ventajas es lo f\'acil de identificar los clusters a diferencia
de un grid donde no existe un modo establecido para lograrlo.

Al comienzo los $N$ patrones son colocados
en las c\'elulas de forma aleatoria. Luego se tienen $na$ hormigas que van a tomar
un pixel de forma aleatoria. Posterior comienza un ciclo donde se selecciona una hormiga
aleatoriamente. Si la hormiga no carga ning\'un pixel, est\'a busca un pixel
que no este siendo cargado por otra hormiga y probabilisticamente decide si agarrarlo o dejarlo.
En el caso contrario, busca una c\'elula donde soltar su pixel e igual que antes
probabil\'isticamente decide si soltarlo o quedarselo. Las hormigas van a poseer
una memoria colectiva o global donde van a recordad donde han soltado los p\'ixeles.
De tal forma primero revisa en estos recuerdo e intenta dejarlo con el que tenga 
mayor probabilidad de hacerlo, sino simplemente trate en una c\'elula
aleatoria. Cada 100 iteraciones se va a eliminar de
el menos usado y de esta forma se va variando la memoria, y se mantienen los que
esten ayudando m\'as/.

Se tiene

\[
f(p_i,c_k) = {1 \over n_k} \sum_{p_j \in c_k} {\alpha^2 \over {\alpha^2 + d(p_i,p_j)^2}}
\]

donde $d_(p_i,p_j)$ es una funci\'on de disimilaridad
entres dos patrones y $\alpha$ es el promedio de todas
las diferencias entre los patrones:

\[
\alpha = {1 \over {N*(N-1)}} \sum_{j=1}^N \sum_{i=j+1}^N {2 \times d(p_i,p_j)}
\]

La probabilidad de agarrar un pixel es:

\[
p_{pick}(p_i,c_k) = 
\begin{cases}
1 & \text{si } |c_k| = 1 \\
q & \text{si } |c_k| = 2 \\
\cos( {\pi \over 2} f(p_i, c_k) )^2  & \text {sino}
\end{cases}
\]

Y la de soltarlo es:

\[
p_{drop}(p_i,c_k) = 1 - \cos( {\pi \over 2} f(p_i, c_k) )^2 
\]

El $q$ usado es el recomendado (0.7). Un psudoc\'odigo es
el siguiente:

\begin{lstlisting}[mathescape, language=Pascal]
  Inicialización.
  Hacer maxit veces
    Se toma una hormiga rn aleatoriamente
	Si rn tiene un pixel
	  Si la memoria no esta vacía
	    Buscar soltarlo en alguno de la memoria
		Sino lo dejo
		  Tratar de dejarlo en una célula aleatoria
	  Sino
	    Tratar de dejarlo en una célula aleatoria
	Sino
	  Tomar un pixel que no cargue ninguna hormiga de forma aleatoria
	  Tratar de tomarlo
  Reconstruir solución
\end{lstlisting}

El \'unico detalle que falta explicar es la parte de reconstruir la soluci\'on.
Primero lo que se hace es colocar en una c\'elula los pixeles
que no han soltado las hormigas. Luego se pasa de las c\'elulas al 
arreglo de soluci\'on usado por todas las metaheur\'isticas. Finalmente
se unen los clusters muy parecidos o que s\'olo tengan un elemento.

Los resultados que se quieren evaluar est'an divididos en tres partes: 
Primero, es necesario verificar la calidad de estimaci'on del par'ametro
de Hurst usando los m'etodos implementados, segundo, es importante medir 
el tiempo necesario para cada estimaci'on y, por 'ultimo, es importante
comprobar si se pueden detectar ataques de denegaci'on de servicio mediante la
variaci'on del par'ametro y el uso del mecanismo de ventanas deslizantes.

La descripci'on del computador utilizado para ejecutar las pruebas se 
presenta en la secci'on \ref{sect:testbed}. Los archivos utilizados y los
resultados de la verificaci'on de la estimaci'on del par'ametro de Hurst y del
tiempo necesario para la estimaci'on se presentan en la secci'on
\ref{sect:validacion}. Estos resultandos muestran qu'e tan
preciso es la implementaci'on de los m'etodos y cu'anto tiempo en promedio
necesita la herramienta para estimar el valor del par'ametro de Hurst en una
ventana. 

\section{Validaci'on de los estimados} \label{sect:validacion}

Se ten'ia que probar que las implementaciones de los 3 m'etodos produc'ian
estimaciones razonables del par'ametro de Hurst para saber de antemano que tan
confiable podrian ser la estimaciones cuando se usasen junto con el mecanismo
de ventanas delizantes. Para la validaci'on de los m'etodos implementados se
probaron los mismos con datos producidos por el algoritmo de Paxson
\cite{Paxson95fastapproximation}.

El algoritmo de Paxson permite crear una serie de tiempo pseudo-aleatoria en
el dominio del tiempo de forma r'apida, y permite especificar el par'ametro de
Hurst a ser simulado y el n'umero de datos que debe tener la serie de tiempo
resultante, manteniendo la media en $0$ y la varianza en $1$, con valores
uniformemente distribuidos en el intervalo $[0,2\pi]$. Mediante el algoritmo
se puede simular un proceso estacionario autosimilar con dependencia de largo
alcance \cite{Paxson95fastapproximation}.

La implementaci'on del algoritmo de Paxson utilizada es la que viene en el 
paquete {\tt fArma} de {\tt R}. La validaci'on cont'o con 6 tama~nos diferentes
para la serie de tiempo $X_k$ generada por el algoritmo de Paxson. Estos 
tama~nos fueron $\{1024, 2048, 4096, 8192, 16384, 32768\}$. Por cada tama~no se
generaron 10 series de tiempo $X_k$ para cada uno de los 9 valores del
par'ametro de Hurst que se quer'ia simular. Estos valores fueron
$\{0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95 \}$. 
\begin{figure}[htb]
\centering
\includegraphics[scale=0.45,type=png,ext=.png,read=.png]{figures/abserror-rs}
\caption{Promedio y varianza del error absoluto de la estimaci'on del par'ametro
de Hurst para el m'etodo estad'istico R/S}
\label{fig:abserrrs}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.45,type=png,ext=.png,read=.png]{figures/abserror-var}
\caption{Promedio y varianza del error absoluto de la estimaci'on del par'ametro
de Hurst para el m'etodo de gr'aficas de varianza-tiempo}
\label{fig:abserrvar}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.45,type=png,ext=.png,read=.png]{figures/abserror-mavar}
\caption{Promedio y varianza del error absoluto de la estimaci'on del par'ametro
de Hurst para el m'etodo de varianza modificada de Allan}
\label{fig:abserrmavar}
\end{figure}

\clearpage

\begin{figure}[htb]
\centering
\includegraphics[scale=0.38,type=png,ext=.png,read=.png]{figures/time-n-plot}
\caption{Tiempo promedio utilizado para estimar una ventana de 1024 a 32768
datos para los m'etodos implementados}
\label{fig:tiempoventana}
\end{figure}

Observando las figuras \ref{fig:abserrrs}, \ref{fig:abserrvar} y
\ref{fig:abserrmavar}, lo que se puede evidenciar es que, en general, el
promedio del error absoluto de las estimaciones del par'ametro de Hurst de los
m'etodos mejoran y la varianza disminuye cuando se toman en cuenta un mayor
n'umero de datos para la ventana de estimaci'on. Sin embargo, para los
dos m'etodos que dependen de c'alculos con la varianza el promedio del error 
absoluto de la estimaci'on, incluso con el mayor n'umero de datos es mayor a
$0.1$. El m'etodo del estad'istico R/S es el m'etodo que se comporta mejora 
ya que tiene, en general, un error absoluto muy cerca a $0$ con una varianza
m'inima. 

De acuerdo a \cite{intelligentfuzzy} si queremos analizar los datos en tiempo
real para una red se tendr'ia que hacer una estimaci'on como m'inimo cada
segundo, por lo que una estimaci'on no puede tardar m'as de un segundo en
realizarse. Esto es seriamente limitante ya que cada m'etodo tiene un tiempo de
ejecuci'on muy diferente. Observando la figura \ref{fig:tiempoventana}, con
esta restricci'on la 'unica posibilidad que se tiene es limitar al m'etodo del
estad'istico R/S a $4096$ datos, al m'etodo de gr'aficas varianza-tiempo a
$16834$ datos y al m'etodo de varianza modificada de Allan a $4096$ datos. Dado
que para comparar los m'etodos es importante usar la misma cantidad de datos,
decidimos trabajar con $4096$ datos. El limitar el n'umero de datos va a
afectar la estimaci'on del par'ametro de Hurst directamente como se muestra en
las figuras \ref{fig:abserrrs}, \ref{fig:abserrvar} y \ref{fig:abserrmavar}. 

A pesar de estos problemas, seguimos adelante con el proyecto porque no
queremos una sola estimaci'on puntual y precisa de la traza, sino detectar su
variaci'on, por lo que optamos por un buen balance entre el tiempo de
ejecuci'on y el n'umero de datos a utilizar \cite{intelligentfuzzy}.


